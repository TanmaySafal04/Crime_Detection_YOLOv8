{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.54  Python-3.11.5 torch-2.3.1+cpu CPU (AMD Ryzen 5 5600X 6-Core Processor)\n",
      "Setup complete  (12 CPUs, 15.9 GB RAM, 570.1/653.9 GB disk)\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting roboflow\n",
      "  Downloading roboflow-1.1.34-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (2024.7.4)\n",
      "Collecting chardet==4.0.0 (from roboflow)\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: idna==3.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (1.4.5)\n",
      "Requirement already satisfied: matplotlib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (1.26.4)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (10.4.0)\n",
      "Requirement already satisfied: python-dateutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (1.0.1)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (2.32.3)\n",
      "Requirement already satisfied: six in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (4.66.4)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from roboflow) (6.0.1)\n",
      "Collecting requests-toolbelt (from roboflow)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting python-magic (from roboflow)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->roboflow) (1.2.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->roboflow) (4.53.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->roboflow) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->roboflow) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->roboflow) (3.3.2)\n",
      "Downloading roboflow-1.1.34-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-magic, opencv-python-headless, chardet, requests-toolbelt, roboflow\n",
      "Successfully installed chardet-4.0.0 opencv-python-headless-4.10.0.84 python-magic-0.4.27 requests-toolbelt-1.0.0 roboflow-1.1.34\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.2.52, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Crime-Detection-1 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35319/35319 [00:01<00:00, 24499.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Crime-Detection-1 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1582/1582 [00:00<00:00, 3466.01it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"udgdsgsfgsmVmK1bPc\")\n",
    "project = rf.workspace(\"faisal-ali-muhamad\").project(\"crime-detection-ftiiz\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:00<00:00, 179MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOv8n, train it on COCO128 for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained YOLOv8n detection model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\GenAI and computer vision\\\\Crime detection app\\\\Crime_Detection_YOLOv8'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.bashrc',\n",
       " '.config',\n",
       " '.docker',\n",
       " '.idea',\n",
       " '.ipython',\n",
       " '.lightning_studio',\n",
       " '.lightningignore',\n",
       " '.sudo_as_admin_successful',\n",
       " '.vscode',\n",
       " '.zcompdump',\n",
       " '.zsh_history',\n",
       " '.zshrc',\n",
       " 'datasets',\n",
       " 'experiments.ipynb',\n",
       " 'main.py',\n",
       " 'yolov8n.pt',\n",
       " '.profile',\n",
       " '.gitconfig',\n",
       " '.jupyter',\n",
       " '.ssh',\n",
       " '.hushlogin',\n",
       " '.condarc',\n",
       " '.local',\n",
       " '.oh-my-zsh',\n",
       " '.nv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /teamspace/studios/this_studio/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.dataset.txt',\n",
       " 'README.roboflow.txt',\n",
       " 'data.yaml',\n",
       " 'data_eng.yaml',\n",
       " 'test',\n",
       " 'train',\n",
       " 'valid',\n",
       " 'yolov8n.pt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.52 ðŸš€ Python-3.10.10 torch-2.2.1+cu121 CUDA:0 (NVIDIA L4, 22700MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data_eng.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011433 parameters, 3011417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /teamspace/studios/this_studio/datasets/train/labels.cache... 55\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /teamspace/studios/this_studio/datasets/valid/labels.cache... 155 \u001b[0m\n",
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      1/100      2.13G      1.399      2.832      1.297         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.654      0.692      0.729      0.459\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      2/100      2.11G      1.339      2.052      1.262         22        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.853      0.364      0.544      0.313\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      3/100       2.1G      1.391      1.977      1.269         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.386      0.368      0.353      0.174\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      4/100      2.11G       1.38      1.841      1.294         24        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217       0.72      0.521      0.647        0.4\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      5/100      2.11G      1.324      1.666      1.245         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.781      0.459      0.698      0.408\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      6/100      2.11G      1.357       1.68      1.282         23        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.702       0.64      0.742      0.446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      7/100       2.1G      1.273      1.469      1.222         11        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.893      0.749      0.876       0.57\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      8/100      2.11G      1.246      1.371      1.223         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.896      0.825       0.89      0.602\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      9/100      2.11G      1.227      1.263      1.187         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.804      0.646       0.78      0.495\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     10/100       2.1G      1.268      1.273      1.232         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.926      0.826      0.931      0.615\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     11/100       2.1G      1.288       1.24      1.239         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.841      0.875      0.896      0.561\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     12/100      2.11G       1.23      1.156      1.188         23        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.881      0.913      0.938       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     13/100      2.11G      1.233      1.137      1.195         13        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.874      0.936      0.943       0.63\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     14/100       2.1G      1.192      1.107      1.205         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.892      0.837      0.921      0.593\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     15/100      2.09G      1.174       1.07      1.172         10        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.899      0.886       0.94      0.639\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     16/100      2.11G      1.148      1.013       1.16         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.933      0.902      0.939      0.648\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     17/100      2.11G      1.154      1.011      1.167         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.823      0.903      0.917      0.649\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     18/100       2.1G       1.13     0.9726      1.159         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.911      0.937      0.956      0.631\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     19/100      2.09G      1.123     0.9596      1.129         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.833      0.878      0.903      0.643\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     20/100      2.11G      1.123      0.968      1.142         22        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.885      0.952       0.95      0.661\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     21/100      2.11G      1.115     0.9375      1.145         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.922      0.922      0.958      0.672\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     22/100       2.1G      1.119      0.911      1.145         27        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.871      0.917      0.949       0.66\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     23/100      2.09G      1.107     0.9121      1.133         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217       0.91      0.893      0.946      0.666\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     24/100      2.11G      1.061     0.8768       1.13         19        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.891      0.916      0.942      0.664\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     25/100       2.1G      1.072     0.8845      1.117         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.931      0.881      0.944      0.674\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     26/100       2.1G       1.08     0.8583      1.132         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.931      0.898      0.951      0.691\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     27/100      2.09G      1.071     0.8639      1.131         21        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.911      0.888      0.937      0.668\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     28/100      2.11G      1.055     0.8318      1.108         21        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.876      0.948      0.953      0.684\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     29/100      2.11G       1.05      0.825      1.098         20        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.936      0.904      0.944      0.683\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     30/100       2.1G      1.057     0.7863      1.119         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.949      0.907      0.959      0.686\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     31/100      2.09G      1.047     0.8162       1.12         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.904      0.877      0.948      0.662\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     32/100      2.11G       1.03     0.8056      1.101         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.917       0.92      0.957      0.697\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     33/100      2.11G      1.023     0.7901       1.11         21        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.901      0.915      0.956      0.667\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     34/100       2.1G      1.054     0.8143      1.116         19        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217       0.92      0.906      0.942      0.676\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     35/100      2.09G      1.001     0.7592      1.106         27        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.907      0.932      0.944      0.682\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     36/100      2.11G      1.032     0.7573      1.104         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.905      0.953      0.958      0.696\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     37/100      2.11G      1.036     0.7799      1.114         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.922      0.927      0.965       0.71\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     38/100       2.1G      1.006     0.7619      1.101         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.908      0.912      0.965        0.7\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     39/100      2.09G      1.002     0.7529      1.095         22        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.916      0.931      0.967      0.714\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     40/100      2.11G      1.005     0.7479      1.101         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.939      0.913      0.971      0.706\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     41/100      2.11G     0.9892     0.7452      1.089         24        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.916      0.944      0.965      0.696\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     42/100       2.1G     0.9919     0.7338      1.088         19        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.891      0.913      0.946       0.67\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     43/100      2.09G     0.9748     0.7281      1.092         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217       0.93      0.903      0.957      0.689\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     44/100      2.11G     0.9862     0.7038      1.091         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.948      0.911      0.965      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     45/100      2.11G     0.9836     0.6826       1.09         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.931      0.924      0.963       0.71\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     46/100       2.1G     0.9961     0.7142      1.095         13        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.906      0.929      0.956       0.67\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     47/100      2.09G     0.9787     0.7137      1.096         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.897      0.933       0.95      0.678\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     48/100      2.11G     0.9324     0.6847      1.068         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.911      0.942      0.966       0.71\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     49/100       2.1G     0.9592     0.6909      1.092         19        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.867      0.949      0.962      0.703\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     50/100       2.1G     0.9463     0.6686      1.073         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.889      0.958      0.954      0.704\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     51/100      2.09G     0.9539     0.6721      1.079         18        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.942      0.928      0.965      0.686\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     52/100      2.11G     0.9498     0.6558      1.067         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.959      0.933      0.965      0.701\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     53/100      2.11G     0.9263     0.6533      1.054         23        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.904      0.945      0.966      0.707\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     54/100       2.1G      0.916     0.6656      1.064         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.949       0.92      0.968      0.706\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     55/100      2.09G     0.9201     0.6532      1.067         13        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.934      0.927      0.967      0.712\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     56/100      2.11G     0.9054     0.6424      1.056         25        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217       0.92      0.944      0.969      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     57/100      2.11G     0.9247     0.6553      1.054         19        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.925       0.94      0.969      0.695\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     58/100       2.1G     0.9702     0.6645        1.1         21        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.936      0.921      0.964      0.701\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     59/100      2.09G     0.9136     0.6375      1.049         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.948      0.921      0.966      0.731\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     60/100       2.1G     0.9101     0.6288      1.039         11        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.937      0.932      0.966      0.722\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     61/100      2.11G     0.9561     0.6413      1.071         20        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.893      0.962      0.955        0.7\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     62/100       2.1G      0.898     0.6184       1.06         33        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.886      0.927      0.942      0.692\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     63/100      2.09G     0.9037     0.6094      1.049         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217       0.88      0.958      0.958      0.681\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     64/100      2.11G     0.8896     0.6121      1.053         18        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.881      0.947      0.958      0.685\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     65/100      2.11G     0.8951     0.6192      1.046         19        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217       0.86      0.941      0.948      0.701\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     66/100       2.1G      0.893       0.62      1.058         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.945      0.914      0.965      0.735\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     67/100      2.09G     0.8764     0.5848      1.045         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.926      0.936      0.962      0.722\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     68/100      2.11G     0.8889     0.6092      1.046         18        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.891      0.953      0.963      0.721\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     69/100      2.11G     0.8925     0.6091      1.049         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.925      0.885      0.963      0.714\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     70/100       2.1G     0.8787      0.589       1.04         20        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.945      0.881      0.957      0.725\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     71/100      2.09G      0.886     0.6229      1.048          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.923      0.934      0.958      0.722\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     72/100      2.11G     0.8524     0.5797      1.027         20        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.954      0.912      0.958      0.715\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     73/100      2.11G     0.8734     0.5811       1.04         13        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.921      0.904      0.957      0.719\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     74/100       2.1G     0.8476     0.5682      1.015         24        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.864      0.954      0.945      0.701\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     75/100      2.09G     0.8504     0.5633       1.03         23        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.875      0.951      0.959      0.729\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     76/100      2.11G     0.8572     0.5878       1.04         13        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.875      0.942      0.949      0.721\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     77/100      2.11G     0.8303     0.5759       1.02         19        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.903      0.927      0.956      0.731\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     78/100       2.1G     0.8681     0.5816      1.043         11        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217       0.93      0.898       0.96      0.731\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     79/100      2.09G      0.843      0.585      1.027         25        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.931        0.9      0.958      0.716\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     80/100      2.11G     0.8307     0.5718      1.026          9        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.938      0.924      0.967      0.727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     81/100      2.11G     0.8308      0.557      1.017         21        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.956      0.897      0.965       0.74\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     82/100       2.1G     0.8421     0.5574      1.021         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.893      0.954      0.961      0.737\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     83/100      2.09G     0.8106     0.5398      1.016         10        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.896       0.95      0.959      0.734\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     84/100      2.11G     0.8011     0.5399      1.015         21        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.928      0.944      0.964      0.736\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     85/100      2.11G     0.8091     0.5383      1.011         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217       0.93      0.944      0.968      0.739\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     86/100       2.1G     0.8239     0.5534      1.017         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.918       0.95      0.967      0.741\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     87/100      2.09G     0.8206     0.5521      1.026         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217       0.93       0.96      0.969      0.753\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     88/100      2.11G     0.8096     0.5204      1.024         13        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.895      0.971      0.965      0.737\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     89/100       2.1G     0.8095     0.5345      1.013         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.924      0.954      0.967      0.745\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     90/100       2.1G      0.798     0.5402      1.011         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.933      0.933      0.968      0.749\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     91/100       2.1G     0.7419     0.4686     0.9873          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.923      0.948      0.967      0.748\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     92/100      2.11G     0.7389     0.4676     0.9928         10        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.933      0.946       0.97      0.746\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     93/100      2.11G      0.728     0.4598     0.9865         18        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.946      0.917      0.964      0.743\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     94/100       2.1G     0.7155     0.4583     0.9773          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.901      0.967      0.967       0.74\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     95/100      2.09G     0.7217     0.4479     0.9924          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217       0.92      0.956      0.966      0.744\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     96/100      2.11G     0.7164     0.4509     0.9888          9        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.922      0.957      0.966      0.741\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     97/100      2.11G     0.7127     0.4382      0.985          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.937      0.946      0.966      0.744\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     98/100       2.1G     0.7081     0.4491     0.9812          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.924      0.947      0.966      0.753\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     99/100      2.09G     0.6999     0.4306     0.9798         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.944       0.94      0.966       0.75\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "    100/100      2.11G     0.6943     0.4375      0.965         10        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.942      0.942      0.967      0.752\n",
      "\n",
      "100 epochs completed in 0.096 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.52 ðŸš€ Python-3.10.10 torch-2.2.1+cu121 CUDA:0 (NVIDIA L4, 22700MiB)\n",
      "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        155        217      0.924      0.947      0.967      0.752\n",
      "                normal         53        102      0.838      0.941      0.945       0.74\n",
      "                 theft         50         50       0.94        0.9       0.96      0.712\n",
      "              shooting         54         65      0.993          1      0.995      0.804\n",
      "Speed: 0.2ms preprocess, 1.0ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
     ]
    }
   ],
   "source": [
    "!yolo train model=yolov8n.pt data=data_eng.yaml epochs=100 imgsz=640 batch=16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GenAI and computer vision\\Crime detection app\\Crime_Detection_YOLOv8\n"
     ]
    }
   ],
   "source": [
    "%cd D:\\GenAI and computer vision\\Crime detection app\\Crime_Detection_YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.idea',\n",
       " '.lightning_studio',\n",
       " '.vscode',\n",
       " 'app.py',\n",
       " 'best.pt',\n",
       " 'datasets',\n",
       " 'Dramatic smash-and-grab robbery at west London jewellers is caught on CCTV.mp4',\n",
       " 'experiments.ipynb']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GenAI and computer vision\\Crime detection app\\Crime_Detection_YOLOv8\\datasets\n"
     ]
    }
   ],
   "source": [
    "%cd D:\\GenAI and computer vision\\Crime detection app\\Crime_Detection_YOLOv8\\datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load a model\n",
    "model = YOLO(\"Crime_best.pt\")  # load our custom model\n",
    "img = 'pencurian_frame_0155_jpg.rf.acd518fe127de05e12c702939fef6a65.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Crime_best.pt',\n",
       " 'data.yaml',\n",
       " 'data_eng.yaml',\n",
       " 'pencurian_frame_0155_jpg.rf.acd518fe127de05e12c702939fef6a65.jpg',\n",
       " 'README.dataset.txt',\n",
       " 'README.roboflow.txt',\n",
       " 'runs',\n",
       " 'test',\n",
       " 'train',\n",
       " 'valid',\n",
       " 'yolov8n.pt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\GenAI and computer vision\\Crime detection app\\Crime_Detection_YOLOv8\\datasets\\pencurian_frame_0155_jpg.rf.acd518fe127de05e12c702939fef6a65.jpg: 640x640 1 theft, 157.5ms\n",
      "Speed: 2.0ms preprocess, 157.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([1.])\n",
       "conf: tensor([0.5581])\n",
       "data: tensor([[290.6464, 126.8779, 387.9673, 400.4509,   0.5581,   1.0000]])\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (640, 640)\n",
       "shape: torch.Size([1, 6])\n",
       "xywh: tensor([[339.3069, 263.6644,  97.3209, 273.5730]])\n",
       "xywhn: tensor([[0.5302, 0.4120, 0.1521, 0.4275]])\n",
       "xyxy: tensor([[290.6464, 126.8779, 387.9673, 400.4509]])\n",
       "xyxyn: tensor([[0.4541, 0.1982, 0.6062, 0.6257]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'normal', 1: 'theft', 2: 'shooting'}\n",
       " obb: None\n",
       " orig_img: array([[[ 14,  14,  14],\n",
       "         [ 14,  14,  14],\n",
       "         [119, 119, 119],\n",
       "         ...,\n",
       "         [161, 157, 156],\n",
       "         [ 20,  18,  17],\n",
       "         [ 15,  13,  12]],\n",
       " \n",
       "        [[ 18,  18,  18],\n",
       "         [ 18,  18,  18],\n",
       "         [124, 124, 124],\n",
       "         ...,\n",
       "         [169, 165, 164],\n",
       "         [ 27,  25,  24],\n",
       "         [ 20,  18,  17]],\n",
       " \n",
       "        [[ 20,  20,  20],\n",
       "         [ 20,  20,  20],\n",
       "         [127, 127, 127],\n",
       "         ...,\n",
       "         [170, 166, 165],\n",
       "         [ 25,  23,  22],\n",
       "         [ 16,  14,  13]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 11,  16,  15],\n",
       "         [ 15,  20,  19],\n",
       "         [ 88,  90,  90],\n",
       "         ...,\n",
       "         [ 24,  22,  22],\n",
       "         [ 20,  18,  18],\n",
       "         [ 17,  15,  15]],\n",
       " \n",
       "        [[ 12,  17,  16],\n",
       "         [ 16,  21,  20],\n",
       "         [ 89,  91,  91],\n",
       "         ...,\n",
       "         [ 19,  17,  17],\n",
       "         [ 18,  16,  16],\n",
       "         [ 18,  16,  16]],\n",
       " \n",
       "        [[ 13,  18,  17],\n",
       "         [ 17,  22,  21],\n",
       "         [ 90,  92,  92],\n",
       "         ...,\n",
       "         [ 16,  14,  14],\n",
       "         [ 16,  14,  14],\n",
       "         [ 19,  17,  17]]], dtype=uint8)\n",
       " orig_shape: (640, 640)\n",
       " path: 'D:\\\\GenAI and computer vision\\\\Crime detection app\\\\Crime_Detection_YOLOv8\\\\datasets\\\\pencurian_frame_0155_jpg.rf.acd518fe127de05e12c702939fef6a65.jpg'\n",
       " probs: None\n",
       " save_dir: 'C:\\\\Users\\\\tanma\\\\runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 2.0017623901367188, 'inference': 157.51075744628906, 'postprocess': 1.0027885437011719}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Results object with attributes:\n",
       "\n",
       "boxes: ultralytics.engine.results.Boxes object\n",
       "keypoints: None\n",
       "masks: None\n",
       "names: {0: 'normal', 1: 'theft', 2: 'shooting'}\n",
       "obb: None\n",
       "orig_img: array([[[ 14,  14,  14],\n",
       "        [ 14,  14,  14],\n",
       "        [119, 119, 119],\n",
       "        ...,\n",
       "        [161, 157, 156],\n",
       "        [ 20,  18,  17],\n",
       "        [ 15,  13,  12]],\n",
       "\n",
       "       [[ 18,  18,  18],\n",
       "        [ 18,  18,  18],\n",
       "        [124, 124, 124],\n",
       "        ...,\n",
       "        [169, 165, 164],\n",
       "        [ 27,  25,  24],\n",
       "        [ 20,  18,  17]],\n",
       "\n",
       "       [[ 20,  20,  20],\n",
       "        [ 20,  20,  20],\n",
       "        [127, 127, 127],\n",
       "        ...,\n",
       "        [170, 166, 165],\n",
       "        [ 25,  23,  22],\n",
       "        [ 16,  14,  13]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 11,  16,  15],\n",
       "        [ 15,  20,  19],\n",
       "        [ 88,  90,  90],\n",
       "        ...,\n",
       "        [ 24,  22,  22],\n",
       "        [ 20,  18,  18],\n",
       "        [ 17,  15,  15]],\n",
       "\n",
       "       [[ 12,  17,  16],\n",
       "        [ 16,  21,  20],\n",
       "        [ 89,  91,  91],\n",
       "        ...,\n",
       "        [ 19,  17,  17],\n",
       "        [ 18,  16,  16],\n",
       "        [ 18,  16,  16]],\n",
       "\n",
       "       [[ 13,  18,  17],\n",
       "        [ 17,  22,  21],\n",
       "        [ 90,  92,  92],\n",
       "        ...,\n",
       "        [ 16,  14,  14],\n",
       "        [ 16,  14,  14],\n",
       "        [ 19,  17,  17]]], dtype=uint8)\n",
       "orig_shape: (640, 640)\n",
       "path: 'D:\\\\GenAI and computer vision\\\\Crime detection app\\\\Crime_Detection_YOLOv8\\\\datasets\\\\pencurian_frame_0155_jpg.rf.acd518fe127de05e12c702939fef6a65.jpg'\n",
       "probs: None\n",
       "save_dir: 'C:\\\\Users\\\\tanma\\\\runs\\\\detect\\\\predict'\n",
       "speed: {'preprocess': 2.0017623901367188, 'inference': 157.51075744628906, 'postprocess': 1.0027885437011719}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bounding boxes, labels, and confidence scores\n",
    "boxes = result[0].boxes.xyxy[0].cpu().numpy()  # (x1, y1, x2, y2, conf, class)\n",
    "labels = result[0].boxes.cls  # class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d=boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     290.65,      126.88,      387.97,      400.45], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126.87793"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, filter results by confidence score threshold\n",
    "confidence_threshold = 0.5\n",
    "filtered_boxes = [box for box in boxes if box[4] > confidence_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype <U64 cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mplot_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[46], line 4\u001b[0m, in \u001b[0;36mplot_results\u001b[1;34m(image, boxes, labels)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_results\u001b[39m(image, boxes, labels):\n\u001b[0;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5581\u001b[39m\n\u001b[0;32m      7\u001b[0m     x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m boxes\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:2695\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2689\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   2690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   2691\u001b[0m         X, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2692\u001b[0m         alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, extent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2693\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filternorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, filterrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m,\n\u001b[0;32m   2694\u001b[0m         resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2695\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2697\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2699\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2701\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2702\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2703\u001b[0m     sci(__ret)\n\u001b[0;32m   2704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:1446\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1446\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1448\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1449\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1450\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:5663\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5656\u001b[0m im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m   5657\u001b[0m                       interpolation\u001b[38;5;241m=\u001b[39minterpolation, origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[0;32m   5658\u001b[0m                       extent\u001b[38;5;241m=\u001b[39mextent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m   5659\u001b[0m                       filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   5660\u001b[0m                       interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5661\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 5663\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5664\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5666\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\image.py:701\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39msafe_masked_invalid(A, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    700\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage data of dtype \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cannot be converted to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    702\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;66;03m# If just one dimension assume scalar and apply colormap\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A[:, :, \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype <U64 cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_results(image, boxes, labels):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    conf = 0.5581\n",
    "    x1, y1, x2, y2 = boxes\n",
    "    cls = int(1)\n",
    "    label = 'theft'\n",
    "        \n",
    "        # Draw bounding box\n",
    "    plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2))\n",
    "        \n",
    "        # Display label and confidence score\n",
    "    plt.text(x1, y1, f'{label} {conf:.2f}', bbox=dict(facecolor='yellow', alpha=0.5), fontsize=12, color='black')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the results\n",
    "plot_results(img, boxes, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VIDEO_PATH_1 = 'Dramatic smash-and-grab robbery at west London jewellers is caught on CCTV.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'yolo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=predict model=model conf=0.25 source={SOURCE_VIDEO_PATH_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
